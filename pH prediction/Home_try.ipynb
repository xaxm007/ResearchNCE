{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 17:47:52.366787: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-23 17:47:52.430288: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-23 17:47:52.431269: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-23 17:47:53.958959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 17:47:59.146739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-23 17:47:59.147325: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2561/2561 [==============================] - 6s 2ms/step - loss: 2.6515 - mae: 1.1834 - val_loss: 1.2998 - val_mae: 0.9950\n",
      "Epoch 2/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 1.2146 - mae: 0.9557 - val_loss: 1.1504 - val_mae: 0.9344\n",
      "Epoch 3/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 1.1103 - mae: 0.8948 - val_loss: 1.1228 - val_mae: 0.8968\n",
      "Epoch 4/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 1.0229 - mae: 0.8458 - val_loss: 1.0402 - val_mae: 0.8660\n",
      "Epoch 5/100\n",
      "2561/2561 [==============================] - 10s 4ms/step - loss: 0.9801 - mae: 0.8221 - val_loss: 1.0064 - val_mae: 0.8373\n",
      "Epoch 6/100\n",
      "2561/2561 [==============================] - 7s 3ms/step - loss: 0.9467 - mae: 0.8071 - val_loss: 1.0118 - val_mae: 0.8421\n",
      "Epoch 7/100\n",
      "2561/2561 [==============================] - 7s 3ms/step - loss: 0.9288 - mae: 0.7998 - val_loss: 0.9465 - val_mae: 0.8169\n",
      "Epoch 8/100\n",
      "2561/2561 [==============================] - 9s 4ms/step - loss: 0.9159 - mae: 0.7924 - val_loss: 0.9682 - val_mae: 0.8065\n",
      "Epoch 9/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.9083 - mae: 0.7914 - val_loss: 0.9433 - val_mae: 0.8059\n",
      "Epoch 10/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.8928 - mae: 0.7858 - val_loss: 0.9376 - val_mae: 0.8041\n",
      "Epoch 11/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.8922 - mae: 0.7860 - val_loss: 0.9219 - val_mae: 0.7962\n",
      "Epoch 12/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.8882 - mae: 0.7844 - val_loss: 0.8896 - val_mae: 0.7932\n",
      "Epoch 13/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.8767 - mae: 0.7798 - val_loss: 0.9237 - val_mae: 0.7896\n",
      "Epoch 14/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.8730 - mae: 0.7785 - val_loss: 0.9304 - val_mae: 0.7985\n",
      "Epoch 15/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.8677 - mae: 0.7762 - val_loss: 0.8695 - val_mae: 0.7841\n",
      "Epoch 16/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.8590 - mae: 0.7710 - val_loss: 0.8791 - val_mae: 0.7826\n",
      "Epoch 17/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.8609 - mae: 0.7717 - val_loss: 0.8690 - val_mae: 0.7847\n",
      "Epoch 18/100\n",
      "2561/2561 [==============================] - 6s 3ms/step - loss: 0.8558 - mae: 0.7720 - val_loss: 0.8921 - val_mae: 0.7860\n",
      "Epoch 19/100\n",
      "2561/2561 [==============================] - 6s 2ms/step - loss: 0.8511 - mae: 0.7678 - val_loss: 0.8665 - val_mae: 0.7790\n",
      "Epoch 20/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.8555 - mae: 0.7717 - val_loss: 0.8642 - val_mae: 0.7824\n",
      "Epoch 21/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.8491 - mae: 0.7690 - val_loss: 0.8691 - val_mae: 0.7824\n",
      "Epoch 22/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.8505 - mae: 0.7688 - val_loss: 0.8539 - val_mae: 0.7795\n",
      "Epoch 23/100\n",
      "2561/2561 [==============================] - 4s 1ms/step - loss: 0.8400 - mae: 0.7645 - val_loss: 0.9295 - val_mae: 0.8027\n",
      "Epoch 24/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.8345 - mae: 0.7601 - val_loss: 0.8632 - val_mae: 0.7816\n",
      "Epoch 25/100\n",
      "2561/2561 [==============================] - 4s 1ms/step - loss: 0.8330 - mae: 0.7615 - val_loss: 0.8461 - val_mae: 0.7721\n",
      "Epoch 26/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.8265 - mae: 0.7575 - val_loss: 0.8514 - val_mae: 0.7753\n",
      "Epoch 27/100\n",
      "2561/2561 [==============================] - 4s 1ms/step - loss: 0.8248 - mae: 0.7566 - val_loss: 0.8235 - val_mae: 0.7597\n",
      "Epoch 28/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.8215 - mae: 0.7553 - val_loss: 0.8360 - val_mae: 0.7617\n",
      "Epoch 29/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.8185 - mae: 0.7545 - val_loss: 0.8388 - val_mae: 0.7687\n",
      "Epoch 30/100\n",
      "2561/2561 [==============================] - 4s 1ms/step - loss: 0.8113 - mae: 0.7507 - val_loss: 0.8744 - val_mae: 0.7707\n",
      "Epoch 31/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.8123 - mae: 0.7503 - val_loss: 0.8228 - val_mae: 0.7622\n",
      "Epoch 32/100\n",
      "2561/2561 [==============================] - 4s 1ms/step - loss: 0.8129 - mae: 0.7520 - val_loss: 0.8164 - val_mae: 0.7588\n",
      "Epoch 33/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.8069 - mae: 0.7494 - val_loss: 0.8202 - val_mae: 0.7566\n",
      "Epoch 34/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.8026 - mae: 0.7468 - val_loss: 0.8577 - val_mae: 0.7624\n",
      "Epoch 35/100\n",
      "2561/2561 [==============================] - 4s 1ms/step - loss: 0.8050 - mae: 0.7497 - val_loss: 0.8322 - val_mae: 0.7628\n",
      "Epoch 36/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.8047 - mae: 0.7481 - val_loss: 0.8127 - val_mae: 0.7596\n",
      "Epoch 37/100\n",
      "2561/2561 [==============================] - 6s 2ms/step - loss: 0.7981 - mae: 0.7459 - val_loss: 0.8140 - val_mae: 0.7578\n",
      "Epoch 38/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.7971 - mae: 0.7443 - val_loss: 0.8026 - val_mae: 0.7547\n",
      "Epoch 39/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.7957 - mae: 0.7448 - val_loss: 0.7940 - val_mae: 0.7506\n",
      "Epoch 40/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7892 - mae: 0.7414 - val_loss: 0.7875 - val_mae: 0.7501\n",
      "Epoch 41/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7885 - mae: 0.7412 - val_loss: 0.8222 - val_mae: 0.7535\n",
      "Epoch 42/100\n",
      "2561/2561 [==============================] - 6s 2ms/step - loss: 0.7859 - mae: 0.7400 - val_loss: 0.8031 - val_mae: 0.7530\n",
      "Epoch 43/100\n",
      "2561/2561 [==============================] - 6s 2ms/step - loss: 0.7849 - mae: 0.7381 - val_loss: 0.8298 - val_mae: 0.7611\n",
      "Epoch 44/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.7810 - mae: 0.7366 - val_loss: 0.7875 - val_mae: 0.7497\n",
      "Epoch 45/100\n",
      "2561/2561 [==============================] - 4s 1ms/step - loss: 0.7790 - mae: 0.7368 - val_loss: 0.8020 - val_mae: 0.7527\n",
      "Epoch 46/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.7798 - mae: 0.7367 - val_loss: 0.7864 - val_mae: 0.7446\n",
      "Epoch 47/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.7736 - mae: 0.7334 - val_loss: 0.7833 - val_mae: 0.7468\n",
      "Epoch 48/100\n",
      "2561/2561 [==============================] - 17s 6ms/step - loss: 0.7750 - mae: 0.7356 - val_loss: 0.7882 - val_mae: 0.7419\n",
      "Epoch 49/100\n",
      "2561/2561 [==============================] - 27s 11ms/step - loss: 0.7721 - mae: 0.7329 - val_loss: 0.7664 - val_mae: 0.7338\n",
      "Epoch 50/100\n",
      "2561/2561 [==============================] - 30s 12ms/step - loss: 0.7654 - mae: 0.7301 - val_loss: 0.8027 - val_mae: 0.7438\n",
      "Epoch 51/100\n",
      "2561/2561 [==============================] - 27s 11ms/step - loss: 0.7647 - mae: 0.7290 - val_loss: 0.7761 - val_mae: 0.7389\n",
      "Epoch 52/100\n",
      "2561/2561 [==============================] - 32s 13ms/step - loss: 0.7665 - mae: 0.7305 - val_loss: 0.8030 - val_mae: 0.7477\n",
      "Epoch 53/100\n",
      "2561/2561 [==============================] - 9s 3ms/step - loss: 0.7606 - mae: 0.7283 - val_loss: 0.7703 - val_mae: 0.7320\n",
      "Epoch 54/100\n",
      "2561/2561 [==============================] - 25s 10ms/step - loss: 0.7586 - mae: 0.7266 - val_loss: 0.7908 - val_mae: 0.7379\n",
      "Epoch 55/100\n",
      "2561/2561 [==============================] - 39s 15ms/step - loss: 0.7570 - mae: 0.7259 - val_loss: 0.7658 - val_mae: 0.7328\n",
      "Epoch 56/100\n",
      "2561/2561 [==============================] - 27s 11ms/step - loss: 0.7573 - mae: 0.7267 - val_loss: 0.7672 - val_mae: 0.7365\n",
      "Epoch 57/100\n",
      "2561/2561 [==============================] - 24s 9ms/step - loss: 0.7548 - mae: 0.7260 - val_loss: 0.8058 - val_mae: 0.7349\n",
      "Epoch 58/100\n",
      "2561/2561 [==============================] - 17s 7ms/step - loss: 0.7543 - mae: 0.7258 - val_loss: 0.8098 - val_mae: 0.7521\n",
      "Epoch 59/100\n",
      "2561/2561 [==============================] - 33s 13ms/step - loss: 0.7541 - mae: 0.7265 - val_loss: 0.7461 - val_mae: 0.7231\n",
      "Epoch 60/100\n",
      "2561/2561 [==============================] - 28s 11ms/step - loss: 0.7504 - mae: 0.7222 - val_loss: 0.7572 - val_mae: 0.7325\n",
      "Epoch 61/100\n",
      "2561/2561 [==============================] - 29s 11ms/step - loss: 0.7484 - mae: 0.7212 - val_loss: 0.7758 - val_mae: 0.7375\n",
      "Epoch 62/100\n",
      "2561/2561 [==============================] - 17s 7ms/step - loss: 0.7506 - mae: 0.7233 - val_loss: 0.7604 - val_mae: 0.7321\n",
      "Epoch 63/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7469 - mae: 0.7219 - val_loss: 0.7450 - val_mae: 0.7234\n",
      "Epoch 64/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7461 - mae: 0.7206 - val_loss: 0.7529 - val_mae: 0.7237\n",
      "Epoch 65/100\n",
      "2561/2561 [==============================] - 6s 2ms/step - loss: 0.7461 - mae: 0.7214 - val_loss: 0.7436 - val_mae: 0.7234\n",
      "Epoch 66/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7442 - mae: 0.7214 - val_loss: 0.7664 - val_mae: 0.7270\n",
      "Epoch 67/100\n",
      "2561/2561 [==============================] - 20s 8ms/step - loss: 0.7438 - mae: 0.7202 - val_loss: 0.7534 - val_mae: 0.7265\n",
      "Epoch 68/100\n",
      "2561/2561 [==============================] - 28s 11ms/step - loss: 0.7438 - mae: 0.7205 - val_loss: 0.7982 - val_mae: 0.7444\n",
      "Epoch 69/100\n",
      "2561/2561 [==============================] - 26s 10ms/step - loss: 0.7383 - mae: 0.7167 - val_loss: 0.7486 - val_mae: 0.7248\n",
      "Epoch 70/100\n",
      "2561/2561 [==============================] - 28s 11ms/step - loss: 0.7347 - mae: 0.7146 - val_loss: 0.7496 - val_mae: 0.7261\n",
      "Epoch 71/100\n",
      "2561/2561 [==============================] - 19s 7ms/step - loss: 0.7335 - mae: 0.7155 - val_loss: 0.7816 - val_mae: 0.7344\n",
      "Epoch 72/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.7325 - mae: 0.7149 - val_loss: 0.7576 - val_mae: 0.7199\n",
      "Epoch 73/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.7340 - mae: 0.7151 - val_loss: 0.7411 - val_mae: 0.7236\n",
      "Epoch 74/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.7286 - mae: 0.7118 - val_loss: 0.7351 - val_mae: 0.7234\n",
      "Epoch 75/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.7323 - mae: 0.7142 - val_loss: 0.7281 - val_mae: 0.7166\n",
      "Epoch 76/100\n",
      "2561/2561 [==============================] - 3s 1ms/step - loss: 0.7282 - mae: 0.7123 - val_loss: 0.7520 - val_mae: 0.7187\n",
      "Epoch 77/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.7271 - mae: 0.7119 - val_loss: 0.7500 - val_mae: 0.7209\n",
      "Epoch 78/100\n",
      "2561/2561 [==============================] - 6s 2ms/step - loss: 0.7289 - mae: 0.7126 - val_loss: 0.7345 - val_mae: 0.7202\n",
      "Epoch 79/100\n",
      "2561/2561 [==============================] - 6s 2ms/step - loss: 0.7230 - mae: 0.7092 - val_loss: 0.7688 - val_mae: 0.7242\n",
      "Epoch 80/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7240 - mae: 0.7104 - val_loss: 0.7519 - val_mae: 0.7252\n",
      "Epoch 81/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.7236 - mae: 0.7105 - val_loss: 0.7419 - val_mae: 0.7190\n",
      "Epoch 82/100\n",
      "2561/2561 [==============================] - 4s 2ms/step - loss: 0.7219 - mae: 0.7097 - val_loss: 0.7203 - val_mae: 0.7139\n",
      "Epoch 83/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7180 - mae: 0.7072 - val_loss: 0.7357 - val_mae: 0.7211\n",
      "Epoch 84/100\n",
      "2561/2561 [==============================] - 7s 3ms/step - loss: 0.7204 - mae: 0.7085 - val_loss: 0.7215 - val_mae: 0.7116\n",
      "Epoch 85/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7191 - mae: 0.7083 - val_loss: 0.7201 - val_mae: 0.7135\n",
      "Epoch 86/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7213 - mae: 0.7095 - val_loss: 0.7437 - val_mae: 0.7174\n",
      "Epoch 87/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7189 - mae: 0.7086 - val_loss: 0.7514 - val_mae: 0.7274\n",
      "Epoch 88/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7163 - mae: 0.7060 - val_loss: 0.7295 - val_mae: 0.7086\n",
      "Epoch 89/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7132 - mae: 0.7053 - val_loss: 0.7758 - val_mae: 0.7348\n",
      "Epoch 90/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7162 - mae: 0.7082 - val_loss: 0.7278 - val_mae: 0.7126\n",
      "Epoch 91/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7116 - mae: 0.7040 - val_loss: 0.7365 - val_mae: 0.7146\n",
      "Epoch 92/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7126 - mae: 0.7049 - val_loss: 0.7622 - val_mae: 0.7268\n",
      "Epoch 93/100\n",
      "2561/2561 [==============================] - 6s 2ms/step - loss: 0.7099 - mae: 0.7022 - val_loss: 0.7334 - val_mae: 0.7066\n",
      "Epoch 94/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7067 - mae: 0.7022 - val_loss: 0.7264 - val_mae: 0.7169\n",
      "Epoch 95/100\n",
      "2561/2561 [==============================] - 6s 2ms/step - loss: 0.7100 - mae: 0.7044 - val_loss: 0.7075 - val_mae: 0.7054\n",
      "Epoch 96/100\n",
      "2561/2561 [==============================] - 5s 2ms/step - loss: 0.7097 - mae: 0.7046 - val_loss: 0.7068 - val_mae: 0.7060\n",
      "Epoch 97/100\n",
      "2561/2561 [==============================] - 18s 7ms/step - loss: 0.7110 - mae: 0.7063 - val_loss: 0.7347 - val_mae: 0.7056\n",
      "Epoch 98/100\n",
      "2561/2561 [==============================] - 37s 14ms/step - loss: 0.7064 - mae: 0.7011 - val_loss: 0.7208 - val_mae: 0.7098\n",
      "Epoch 99/100\n",
      "2561/2561 [==============================] - 37s 14ms/step - loss: 0.7085 - mae: 0.7032 - val_loss: 0.7212 - val_mae: 0.7095\n",
      "Epoch 100/100\n",
      "2561/2561 [==============================] - 37s 14ms/step - loss: 0.7054 - mae: 0.7017 - val_loss: 0.7252 - val_mae: 0.7106\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.7098 - mae: 0.7017\n",
      "Test Loss: 0.7098284363746643\n",
      "Test MAE: 0.7017423510551453\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = pd.read_excel(\"upper_shuffled_output.xlsx\")\n",
    "# Load your dataset (assuming it's in a pandas DataFrame called 'data')\n",
    "X = data.drop(columns=['pH'])\n",
    "y = data['pH']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the feature values\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and two hidden layers\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=10, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test MAE:\", test_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251/251 [==============================] - 3s 11ms/step\n",
      "       Actual pH  Predicted pH\n",
      "10138         10      8.380247\n",
      "13088          7      7.438499\n",
      "5567          10      9.548359\n",
      "30901          7      8.419658\n",
      "27607          8      8.305968\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions and actual values into a DataFrame for easier comparison\n",
    "comparison_df = pd.DataFrame({'Actual pH': y_test, 'Predicted pH': y_pred.flatten()})\n",
    "\n",
    "# Display the first few rows of the comparison\n",
    "print(comparison_df.head())\n",
    "\n",
    "# Plot actual vs predicted values to visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, color='r')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='blue', linewidth=2)  # Line of perfect prediction\n",
    "plt.title('Actual vs Predicted pH')\n",
    "plt.xlabel('Actual pH')\n",
    "plt.ylabel('Predicted pH')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [00h 03m 07s]\n",
      "val_loss: 0.7621303200721741\n",
      "\n",
      "Best val_loss So Far: 0.7621303200721741\n",
      "Total elapsed time: 00h 18m 29s\n",
      "\n",
      "Search: Running Trial #7\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "15                |31                |units\n",
      "3                 |2                 |num_layers\n",
      "37                |7                 |units_0\n",
      "0.012802          |0.0054495         |learning_rate\n",
      "55                |25                |units_1\n",
      "89                |45                |units_2\n",
      "\n",
      "Epoch 1/50\n",
      "801/801 [==============================] - 3s 2ms/step - loss: 1.7144 - mae: 1.0377 - val_loss: 2.1743 - val_mae: 1.2041\n",
      "Epoch 2/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 1.1358 - mae: 0.8921 - val_loss: 0.9893 - val_mae: 0.8409\n",
      "Epoch 3/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 1.0194 - mae: 0.8419 - val_loss: 0.9983 - val_mae: 0.8428\n",
      "Epoch 4/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9813 - mae: 0.8247 - val_loss: 1.0924 - val_mae: 0.8733\n",
      "Epoch 5/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9438 - mae: 0.8097 - val_loss: 0.9597 - val_mae: 0.8314\n",
      "Epoch 6/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 1.0024 - mae: 0.8396 - val_loss: 0.9660 - val_mae: 0.8230\n",
      "Epoch 7/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9294 - mae: 0.8030 - val_loss: 0.9452 - val_mae: 0.8155\n",
      "Epoch 8/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9610 - mae: 0.8230 - val_loss: 1.0120 - val_mae: 0.8562\n",
      "Epoch 9/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9988 - mae: 0.8338 - val_loss: 1.1579 - val_mae: 0.9095\n",
      "Epoch 10/50\n",
      "801/801 [==============================] - 2s 3ms/step - loss: 0.9715 - mae: 0.8249 - val_loss: 0.9294 - val_mae: 0.8057\n",
      "Epoch 11/50\n",
      "801/801 [==============================] - 3s 3ms/step - loss: 0.8832 - mae: 0.7891 - val_loss: 0.8318 - val_mae: 0.7816\n",
      "Epoch 12/50\n",
      "801/801 [==============================] - 2s 3ms/step - loss: 0.9344 - mae: 0.8114 - val_loss: 0.9679 - val_mae: 0.8136\n",
      "Epoch 13/50\n",
      "801/801 [==============================] - 3s 4ms/step - loss: 0.9306 - mae: 0.8061 - val_loss: 1.0210 - val_mae: 0.8577\n",
      "Epoch 14/50\n",
      "801/801 [==============================] - 4s 5ms/step - loss: 1.0180 - mae: 0.8374 - val_loss: 1.0208 - val_mae: 0.8546\n",
      "Epoch 15/50\n",
      "801/801 [==============================] - 3s 4ms/step - loss: 0.9711 - mae: 0.8192 - val_loss: 1.1949 - val_mae: 0.9103\n",
      "Epoch 16/50\n",
      "801/801 [==============================] - 2s 3ms/step - loss: 1.2378 - mae: 0.9708 - val_loss: 1.2011 - val_mae: 0.9617\n",
      "Epoch 17/50\n",
      "801/801 [==============================] - 2s 3ms/step - loss: 1.2078 - mae: 0.9532 - val_loss: 1.2075 - val_mae: 0.9481\n",
      "Epoch 18/50\n",
      "801/801 [==============================] - 2s 3ms/step - loss: 1.0772 - mae: 0.8814 - val_loss: 0.9827 - val_mae: 0.8363\n",
      "Epoch 19/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9449 - mae: 0.8158 - val_loss: 0.8967 - val_mae: 0.8025\n",
      "Epoch 20/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9289 - mae: 0.8127 - val_loss: 0.9360 - val_mae: 0.8120\n",
      "Epoch 21/50\n",
      "801/801 [==============================] - 3s 4ms/step - loss: 0.9631 - mae: 0.8316 - val_loss: 1.0341 - val_mae: 0.7990\n",
      "Epoch 22/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9614 - mae: 0.8274 - val_loss: 0.9423 - val_mae: 0.8228\n",
      "Epoch 23/50\n",
      "801/801 [==============================] - 4s 4ms/step - loss: 0.9429 - mae: 0.8259 - val_loss: 0.9836 - val_mae: 0.8183\n",
      "Epoch 24/50\n",
      "801/801 [==============================] - 3s 4ms/step - loss: 0.9357 - mae: 0.8251 - val_loss: 0.9497 - val_mae: 0.8194\n",
      "Epoch 25/50\n",
      "801/801 [==============================] - 3s 4ms/step - loss: 0.9332 - mae: 0.8228 - val_loss: 0.9528 - val_mae: 0.8164\n",
      "Epoch 26/50\n",
      "801/801 [==============================] - 4s 5ms/step - loss: 0.9374 - mae: 0.8234 - val_loss: 0.9831 - val_mae: 0.8507\n",
      "Epoch 27/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9291 - mae: 0.8197 - val_loss: 0.9626 - val_mae: 0.8495\n",
      "Epoch 28/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9222 - mae: 0.8182 - val_loss: 0.9363 - val_mae: 0.8314\n",
      "Epoch 29/50\n",
      "801/801 [==============================] - 2s 3ms/step - loss: 0.9267 - mae: 0.8192 - val_loss: 0.9893 - val_mae: 0.8554\n",
      "Epoch 30/50\n",
      "801/801 [==============================] - 4s 5ms/step - loss: 0.9230 - mae: 0.8164 - val_loss: 0.9357 - val_mae: 0.8291\n",
      "Epoch 31/50\n",
      "801/801 [==============================] - 3s 3ms/step - loss: 0.9507 - mae: 0.8233 - val_loss: 0.9630 - val_mae: 0.8373\n",
      "Epoch 32/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9379 - mae: 0.8232 - val_loss: 0.9573 - val_mae: 0.8410\n",
      "Epoch 33/50\n",
      "801/801 [==============================] - 1s 2ms/step - loss: 0.8908 - mae: 0.8070 - val_loss: 0.8858 - val_mae: 0.8128\n",
      "Epoch 34/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9152 - mae: 0.8143 - val_loss: 0.8934 - val_mae: 0.8085\n",
      "Epoch 35/50\n",
      "801/801 [==============================] - 2s 3ms/step - loss: 0.9029 - mae: 0.8093 - val_loss: 1.0447 - val_mae: 0.8096\n",
      "Epoch 36/50\n",
      "801/801 [==============================] - 3s 4ms/step - loss: 0.8843 - mae: 0.8014 - val_loss: 0.9340 - val_mae: 0.8172\n",
      "Epoch 37/50\n",
      "801/801 [==============================] - 1s 2ms/step - loss: 0.9108 - mae: 0.8137 - val_loss: 0.9455 - val_mae: 0.8407\n",
      "Epoch 38/50\n",
      "801/801 [==============================] - 1s 2ms/step - loss: 0.9370 - mae: 0.8245 - val_loss: 1.0413 - val_mae: 0.8774\n",
      "Epoch 39/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9164 - mae: 0.8136 - val_loss: 0.9358 - val_mae: 0.8154\n",
      "Epoch 40/50\n",
      "801/801 [==============================] - 2s 3ms/step - loss: 0.9116 - mae: 0.8128 - val_loss: 0.9129 - val_mae: 0.8049\n",
      "Epoch 41/50\n",
      "801/801 [==============================] - 2s 2ms/step - loss: 0.9250 - mae: 0.8165 - val_loss: 0.9098 - val_mae: 0.8229\n",
      "Epoch 42/50\n",
      "801/801 [==============================] - 3s 3ms/step - loss: 0.9108 - mae: 0.8111 - val_loss: 1.0083 - val_mae: 0.8587\n",
      "Epoch 43/50\n",
      "801/801 [==============================] - 3s 3ms/step - loss: 0.9675 - mae: 0.8420 - val_loss: 1.1200 - val_mae: 0.9144\n",
      "Epoch 44/50\n",
      "801/801 [==============================] - 2s 3ms/step - loss: 0.9519 - mae: 0.8334 - val_loss: 0.9656 - val_mae: 0.8410\n",
      "Epoch 45/50\n",
      "801/801 [==============================] - 20s 25ms/step - loss: 0.9034 - mae: 0.8127 - val_loss: 0.9322 - val_mae: 0.8309\n",
      "Epoch 46/50\n",
      "801/801 [==============================] - 15s 19ms/step - loss: 0.9167 - mae: 0.8174 - val_loss: 1.0191 - val_mae: 0.8470\n",
      "Epoch 47/50\n",
      "801/801 [==============================] - 14s 18ms/step - loss: 0.9321 - mae: 0.8276 - val_loss: 0.9544 - val_mae: 0.8448\n",
      "Epoch 48/50\n",
      "801/801 [==============================] - 10s 12ms/step - loss: 0.9305 - mae: 0.8242 - val_loss: 0.9744 - val_mae: 0.8347\n",
      "Epoch 49/50\n",
      "801/801 [==============================] - 16s 19ms/step - loss: 0.9450 - mae: 0.8309 - val_loss: 0.9263 - val_mae: 0.8326\n",
      "Epoch 50/50\n",
      "801/801 [==============================] - 14s 18ms/step - loss: 0.9171 - mae: 0.8200 - val_loss: 0.9443 - val_mae: 0.8375\n",
      "Epoch 1/50\n",
      "801/801 [==============================] - 23s 18ms/step - loss: 1.7431 - mae: 1.0440 - val_loss: 1.4389 - val_mae: 1.0111\n",
      "Epoch 2/50\n",
      "801/801 [==============================] - 18s 22ms/step - loss: 1.1827 - mae: 0.9095 - val_loss: 1.1721 - val_mae: 0.8928\n",
      "Epoch 3/50\n",
      "801/801 [==============================] - 24s 31ms/step - loss: 1.0364 - mae: 0.8504 - val_loss: 0.9883 - val_mae: 0.8338\n",
      "Epoch 4/50\n",
      "801/801 [==============================] - 19s 24ms/step - loss: 1.0051 - mae: 0.8403 - val_loss: 0.9725 - val_mae: 0.8347\n",
      "Epoch 5/50\n",
      "801/801 [==============================] - 16s 20ms/step - loss: 0.9715 - mae: 0.8282 - val_loss: 1.3417 - val_mae: 0.9866\n",
      "Epoch 6/50\n",
      "801/801 [==============================] - 16s 19ms/step - loss: 1.0154 - mae: 0.8538 - val_loss: 1.0210 - val_mae: 0.8463\n",
      "Epoch 7/50\n",
      "801/801 [==============================] - 20s 25ms/step - loss: 0.9543 - mae: 0.8255 - val_loss: 0.9099 - val_mae: 0.8159\n",
      "Epoch 8/50\n",
      "801/801 [==============================] - 19s 23ms/step - loss: 0.9439 - mae: 0.8209 - val_loss: 1.0588 - val_mae: 0.8366\n",
      "Epoch 9/50\n",
      "801/801 [==============================] - 26s 33ms/step - loss: 0.9212 - mae: 0.8128 - val_loss: 1.1160 - val_mae: 0.9014\n",
      "Epoch 10/50\n",
      "801/801 [==============================] - 17s 21ms/step - loss: 0.9517 - mae: 0.8245 - val_loss: 1.0244 - val_mae: 0.8659\n",
      "Epoch 11/50\n",
      "680/801 [========================>.....] - ETA: 1s - loss: 0.9493 - mae: 0.8226"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     48\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[1;32m     49\u001b[0m     build_model,                \u001b[38;5;66;03m# Model-building function\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,        \u001b[38;5;66;03m# Target metric to optimize\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpH_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Project name for logs\u001b[39;00m\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Perform hyperparameter search\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Retrieve the best model\u001b[39;00m\n\u001b[1;32m     61\u001b[0m best_model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_models(num_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras/src/engine/training.py:1748\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1746\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1747\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1748\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras/src/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras/src/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras/src/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras/src/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras/src/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras/src/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m     logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m-> 1170\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras/src/utils/generic_utils.py:296\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    293\u001b[0m         info \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m info\n\u001b[0;32m--> 296\u001b[0m     \u001b[43mio_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/keras/src/utils/io_utils.py:80\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(message)\n\u001b[0;32m---> 80\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(message)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/site-packages/ipykernel/iostream.py:609\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.16/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "\n",
    "# Assume data is already loaded into X (features) and y (target)\n",
    "# Split data into train and test sets\n",
    "data = pd.read_excel(\"upper_shuffled_output.xlsx\")\n",
    "# Load your dataset (assuming it's in a pandas DataFrame called 'data')\n",
    "X = data.drop(columns=['pH'])\n",
    "y = data['pH']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the model-building function for Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Tune the number of units in the first layer\n",
    "    model.add(Dense(units=hp.Int('units', min_value=5, max_value=50, step=2),\n",
    "                    activation='relu', input_shape=(X_train_scaled.shape[1],)))\n",
    "\n",
    "    # Tune the number of hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):  # 1 to 3 hidden layers\n",
    "        model.add(Dense(units=hp.Int(f'units_{i}', min_value=5, max_value=100, step=2),\n",
    "                        activation='relu'))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Tune the learning rate for Adam optimizer\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=0.1, sampling='log')\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='mean_squared_error',  # Since it's regression\n",
    "                  metrics=['mae'])  # Mean Absolute Error\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize the Keras Tuner with Random Search\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,                # Model-building function\n",
    "    objective='val_loss',        # Target metric to optimize\n",
    "    max_trials=10,               # Number of different hyperparameter combinations to try\n",
    "    executions_per_trial=2,      # Number of times to train each model (to reduce variance)\n",
    "    directory='tuner_results',   # Directory where the tuner logs will be saved\n",
    "    project_name='pH_prediction' # Project name for logs\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(X_train_scaled, y_train, epochs=50, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_mae = best_model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test MAE:\", test_mae)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters:\", best_hyperparameters.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ayyp/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: keras in /home/ayyp/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from keras-tuner) (2.13.1)\n",
      "Requirement already satisfied: packaging in /home/ayyp/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from keras-tuner) (24.1)\n",
      "Collecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ayyp/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from requests->keras-tuner) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ayyp/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ayyp/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from requests->keras-tuner) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ayyp/.pyenv/versions/3.8.16/lib/python3.8/site-packages (from requests->keras-tuner) (2.2.3)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/home/ayyp/.pyenv/versions/3.8.16/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
